\section{Relation to Bayesian Descent}

The most simple BLUE one can think of is given by

\begin{lemma}[One-Step Zero-Order BLUE]\label{lem: one-step zero-order blue}
	For an arbitrary square integrable random field \(\Loss\) with covariance
	function
	\begin{equation}\label{def: covariance function}
		\C(x,y):= \Cov(\Loss(x), \Loss(y)),
	\end{equation}
	we have
	\begin{equation*}
		\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
		= \E[\Loss(\param + \step)]
		+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
		\big[\Loss(\param) - \E[\Loss(\param)]\big].
	\end{equation*}
\end{lemma}
\fxwarning*{proof}

This isn't a particularly useful approximation. If we were to assume constant
expectation \(\E[\Loss(\param)] = \mu\), then we have
\begin{equation}\label{eq: constant expectation simple blue}
	\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
	=\mu 
	+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
	\big[\Loss(\param) - \mu\big]
\end{equation}
Now if \(\Loss(\param) < \mu\), then minimizing the BLUE in
\eqref{eq: constant expectation simple blue} results in a maximization of
\(\C(\param+\step, \param)\). This will generally be just \(\step=0\) unless
there is some higher variance region. In other words: We are satisfied and
stop optimizing as soon as we get below the average \(\mu\).

Minimizing the BLUE here is even greedier/conservative than maximizing
expected improvement \eqref{eq: expected improvement}. While the expected
improvement takes into account the fact that we can just return to the previous
minimum and therefore forces the improvement to be greater \(0\), we are
essentially optimizing without the \((\cdot)^+\) here. I.e instead of
maximizing
\begin{equation*}
	\EI(\param) = \E\big[
			[\hat{\Loss}_n^*- \Loss(\param)]^+
			\bigm|
			\Loss(\param_1),\dots, \Loss(\param_n)
		\big]
\end{equation*}
we do the following (keep Lemma~\ref{lem: blue is cond. expectation} in mind)
\begin{equation*}
	\max_{\param} \E\big[
		\hat{\Loss}_n^*- \Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big]
	= \hat{\Loss}_n^* - \min_{\param} \E\big[
		\Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big].
\end{equation*}
So if we minimize the BLUE/conditional expectation directly, we essentially fear
regressing back to a worse state as well, making use even more exploration
averse.

The point of this section was to demonstrate that minimizing the
\(\BLUE\) is not a sophisticated optimization method. Yet, we have seen that it
results in a gradient descent method with a specific learning rate. The main
reason why this method is interesting is, that it scales much better with higher
dimensions and is much cheaper. But there is also the fact, that in a stochastic
gradient setting we cannot simply go back to the minimum so far because we do
not know how big the real loss was at any step. So in this sense it actually
makes sense to fear regression to a worse state.