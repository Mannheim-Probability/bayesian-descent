\section{First Order BLUE}

\subsection{Intrinsic Stationarity}

\begin{lemma}\label{lem: blue centered, intrinsically stationary}
	If \(\Loss\) is a \textbf{centered, intrinsically stationary, differentiable}
	random field, with variogram \(\variogram\), then
	\begin{align*}
		\BLUE[\underbrace{\Loss(\param+\step) - \Loss(\param)}_{=:\Loss_\param(\step)}
		\mid \grad\Loss(\param)]
		&=  \grad\variogram(\step)^T[\grad^2\variogram(0)]^{-1}
		\grad\Loss(\param)\\
		\intertext{
			if the variogram is \textbf{rotation invariant}, i.e.
			\(\variogram(\step)=\phi(\|\step^2\|)\), this simplifies to
		}
		&= \frac{\phi'(\|\step\|^2)}{\phi'(0)}
		\langle \step, \grad\Loss(\param)\rangle
	\end{align*}
\end{lemma}
\begin{remark}
	If \(\Loss\) is stationary we have \(\gamma(\step) = \C(0) - \C(\step)\)
	Therefore \(\grad\gamma = - \grad\C\) and \(\grad^2\gamma = -\grad^2\C\)
	which means we can replace the variogram with the covariance in this case.
\end{remark}


\begin{proof}
	As we know \(\Loss\) is centered, we know that any element
	\(\hat{\Loss}(\param+\step)\) from \(\LUE\) has the form \(\langle b,
	\grad\Loss(\param)\rangle\) now
	\begin{align*}
		g(b) 
		&:= \E[\|\Loss(\param+\step) - \langle b, \grad\Loss(\param)\rangle\|^2]\\
		&= \E[\Loss_\param(\step)^2]
		- 2\langle b, \E[\Loss_\param(\step)\grad\Loss(\param)]\rangle
		+ b^T \E[\grad\Loss(\param) \grad\Loss(\param)^T] b\\
		\overset{\text{Lem~\ref{lem: variogram and derivative}}}&=
		2\variogram(\step) - 2\langle b, \grad\variogram(\step)\rangle
		+ b^T \grad^2\variogram(0) b
	\end{align*}
	The first order condition \(\grad g(b) = 0\) results in
	\begin{equation*}
		b = [\grad^2\variogram(0)]^{-1}\grad\variogram(\step).
	\end{equation*}
	This is a minimum since \(\grad^2 g(b)=\grad^2\variogram(0)\) is positive
	definite, due to
	\begin{equation*}
		b^T \grad^2\variogram(0) b = 
		\E[\langle b, \grad\Loss(\param)\rangle^2]\ge 0.
	\end{equation*}
	Now we just need to address the \(\variogram(\step) = \phi(\|\step\|^2)\) case. We have
	\begin{align*}
		\grad \variogram(\step) &= 2\phi'(\|\step\|^2) \step\\
		\grad^2 \variogram(\step)
		&= 4 \phi''(\|\step\|^2) \step \step^T + 2\phi'(\|\step\|^2)\identity
	\end{align*}
	and therefore \(\grad^2\variogram(0) = 2\phi'(0)\identity\). Plugging these results
	into the general formula yields the claim.
\end{proof}

\subsection{Stationarity}

A particularly charming thing about stationarity is, that
\[
	\Cov(\Loss(\param), \grad\Loss(\param))=0,
\]
(cf. Remark~\ref{rem: covariance uncorrelated}). So we can easily ``condition''
on both \(\Loss(\param)\) and \(\grad\Loss(\param)\).

\begin{lemma}\label{lem: blue centered, stationary}
	If \(\Loss\) is a \textbf{stationary, centered, differentiable} random field, then
	\begin{align*}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \grad\Loss(\param)]
		&= \frac{\C(\step)}{\C(0)} \Loss(\param)
		+ \grad\C(\step)^T [\grad^2\C(0)]^{-1} \grad\Loss(\param)
	\\
	\intertext{
		if additionally \(\Loss\) is \textbf{isotropic}, i.e.
		\(\C(\step)=\sqC(\|\step\|^2)\), this reduces to
	}
		&= \frac{\sqC(\|\step\|^2)}{\sqC(0)} \Loss(\param)
		+ \frac{\sqC'(\|\step\|^2)}{\sqC'(0)} \langle \step, \grad\Loss(\param)\rangle.
	\end{align*}
\end{lemma}
\begin{remark}
	One could also use the gradient of the loss exclusively
	\begin{align*}
		\BLUE[\Loss(\param + \step)\mid \grad\Loss(\param)]
		&= [\grad^2\C(0)]^{-1}\langle\grad\C(\step), \grad\Loss(\param)\rangle\\
		&= \frac{\sqC'(\|\step\|^2)}{\sqC'(0)}\langle \step, \grad\Loss(\param)\rangle.
	\end{align*}
	I.e. then the estimator does not take the current height into account.
	Since \(\BLUE[\Loss(\param)\mid \grad\Loss(\param)]=0\) the same holds for
	the loss difference.
\end{remark}
\begin{proof}
	As we are assuming a centred \(\Loss\), we know that any  element
	\(\hat{\Loss}(\param + \step)\) from \(\LUE\) has the form
	\begin{equation*}
		\hat{\Loss}(\param + \step)
		= a\Loss(\param) + \langle b, \grad\Loss(\param)\rangle
	\end{equation*}
	for \(a\in\real\), \(b\in \real^\dimension\). So we want to minimize 
	\begin{align*}
		g(a,b)
		&=\E[\|\Loss(\param +\step) - \hat{\Loss}(\param + \step)\|^2]\\
		&= \begin{aligned}[t]
			&\E[\Loss(\param +\step)^2]\\
			&- 2a\E[\Loss(\param + \step)\Loss(\param)] + a^2 \E[\Loss(\param)^2]\\
			&- 2\langle b, \E[\Loss(\param + \step)\grad\Loss(\param)]\rangle
			+ \langle b, \E[\grad\Loss(\param)\grad\Loss(\param)^T] b\rangle\\
			&- 2a\langle b, \underbrace{\E[\Loss(\param)\grad\Loss(\param)]}_{
				=0\mathrlap{\text{ (Remark~\ref{rem: covariance uncorrelated})}}
			}\rangle
		\end{aligned}\\
		&=\C(0) - 2a\C(\step) + a^2 \C(0)
		- 2\langle b, \grad\C(\step)\rangle
		+ \langle b, \grad^2\C(0) b\rangle
	\end{align*}
	with respect to \(a\) and \(b\). Fortunately there are no interactions
	between \(a\) and \(b\) as \(\Loss(\param)\) is uncorrelated from
	\(\grad\Loss(\param)\). First order conditions result in
	\begin{equation*}
		a= \frac{\C(\step)}{\C(0)}
		\quad \text{and} \quad
		b = [\grad^2 \C(0)]^{-1}\grad \C(\step).
	\end{equation*}
	These are minima, since \(\grad^2\C(0)\) is positive definite, because of
	\begin{align*}
		\langle b, \grad^2\C(0) b\rangle = \E[\langle b, \grad\Loss(\param)\rangle^2] \ge 0.
	\end{align*}
	Now we just need to address the \(\C(\step) = \sqC(\|\step\|^2)\) case. We have
	\begin{align*}
		\grad \C(\step) &= 2\sqC'(\|\step\|^2) \step\\
		\grad^2 \C(\step) &= 4 \sqC''(\|\step\|^2) \step \step^T + 2\sqC'(\|\step\|^2)\identity
	\end{align*}
	and therefore \(\grad^2\C(0) = 2\sqC'(0)\identity\). Plugging these results
	into the general formula yields the claim.
\end{proof}

\subsection{Minimizing the BLUE in the Rotation Invariant Case}

\begin{lemma}\label{lem: rotation invariant implies GD}
	Functions of the form
	\begin{equation*}
		B(\step)
		= g_0(\|\step\|^2) + g_1(\|\step\|^2)\langle \step, \grad\Loss(\param)\rangle
	\end{equation*}
	are minimized by a gradient step, i.e.
	\begin{align}
		-\hat{\lr}\frac{\grad\Loss(\param)}{\|\grad\Loss(\param)\|}
		&= \argmin_{\step}B(\step)\\
		\hat{\lr} &\begin{aligned}[t]
			&=\argmin_{\lr} B\left(-\lr \tfrac{\grad\Loss(\param)}{\|\grad\Loss(\param)\|}\right)\\
			&= \argmin_{\lr} g_0(\lr^2) -  g_1(\lr^2)\lr\|\grad\Loss(\param)\|
		\end{aligned}
	\end{align}
\end{lemma}

\begin{proof}
	Essentially all the occurrences of \(\step\) in \(B\) are
	rotation invariant, so only the step size matters. The only place where that
	is not the case is the scalar product with the gradient. Therefore it
	determines the direction. i.e.
	\begin{align*}
		\min_{\step} B(\step)
		&= \min_{\lr\ge 0}\min_{\step:\|\step\|=\lr} B(\step)
		= \min_{\lr\ge 0}\min_{\step:\|\step\|=\lr}
		g_0(\|\step\|^2)	
		+ g_1(\|\step\|^2) \langle \step, \grad\Loss(\param)\rangle\\
		&= \min_{\lr\ge 0}
		\begin{cases}
			g_0(\lr^2)	
			+ g_1(\lr^2)
			\underbrace{\min_{\step:\|\step\|=\lr}\langle \step, \grad\Loss(\param)\rangle}_{
				= -\eta \|\grad \Loss(\param)\| \quad \mathrlap{\text{(Cauchy-Schwarz)}}
			}\hphantom{\text{Schwarz}} & g_1(\lr^2) \ge 0
			\\
			g_0(\lr^2)	
			+ g_1(\lr^2)
			\underbrace{\max_{\step:\|\step\|=\lr}\langle \step, \grad\Loss(\param)\rangle}_{
				= \eta \|\grad \Loss(\param)\| \quad \mathrlap{\text{(Cauchy-Schwarz)}}
			}\hphantom{\text{Schwarz}} & g_1(\lr^2) < 0
		\end{cases}\\
		&= \min_{\lr} g_0(\lr^2)	
			- g_1(\lr^2)\lr \|\grad\Loss(\param)\|.
		\qedhere
	\end{align*}
\end{proof}

\begin{theorem}[Bayesian Descent]\label{thm: bayesian descent}
	Gradient descent, i.e. 
	\begin{equation*}
		\step(\lr) = - \tilde{\lr} \grad\Loss(\param)
		= - \frac{\lr}{\|\grad\Loss(\param)\|} \grad\Loss(\param)
	\end{equation*}
	minimizes the 1-step 0-order \(\BLUE\) of rotation invariant random fields
	\(\Loss\) in the following sense
	\begin{enumerate}
		\item for a \textbf{centered, intrinsically stationary, differentiable}
		random field \(\Loss\) with \textbf{rotation invariant} variogram
		\(\variogram(\step) = \phi(\|\step\|^2)\), we have
		\begin{align*}
			\step(\hat{\lr})
			&= \argmin_{d}
			\BLUE[\Loss(\param + \step) - \Loss(\param)\mid \grad\Loss(\param)]\\
			\hat{\lr}
			&= \argmax_{\lr} \lr\frac{\phi'(\lr^2)}{\phi'(0)}
		\end{align*}


		\item for a \textbf{centered, isotropic} random field \(\Loss\) with
		\(\C(\step)=C(\|\step\|^2)\), we have
		\begin{align*}
			\step(\hat{\lr})
			&= \argmin_{d}
			\BLUE[\Loss(\param + \step)\mid \Loss(\param), \grad\Loss(\param)]\\
			\hat{\lr}
			&= \argmin_{\lr}\frac{\sqC(\lr^2)}{\sqC(0)} \frac{\Loss(\param)}{\|\grad\Loss(\param)\|}
		-  \lr\frac{\sqC'(\lr^2)}{\sqC'(0)}
		\end{align*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	We simply apply Lemma~\ref{lem: rotation invariant implies GD}	to the results
	from Lemma~\ref{lem: blue centered, intrinsically stationary} and \ref{lem:
	blue centered, stationary} and note that the minimizer does not change if
	we divide the minimization problem by \(\|\grad\Loss(\param)\|\).
\end{proof}

\subsection{Scale Invariance}

A desirable property of optimization algorithms is `scale invariance'. I.e.
the algorithm should result in the same steps if applied to
\(\tilde{\Loss}=c\Loss\) for some \(c\in\real\). This is the case for the
Newton method but not for the standard gradient descent method. But if we have
a closer look at Theorem~\ref{thm: bayesian descent}, we see that
\(\step(\lr)\) does not change under scaling as the constant would appear both
in the gradient and the denominator with the norm around the gradient. So
if \(\hat{\lr}\) stays the same, we would get the exact same step. But
due to
\begin{equation*}
	\tilde{\C}(x,y) = \Cov(c\Loss(x), c\Loss(y)) = c^2 \C(x,y)
\end{equation*}
we have
\begin{equation*}
	\frac{\tilde{\sqC}(\lr^2)}{\tilde{\sqC}(0)}=\frac{\sqC(\lr^2)}{\sqC(0)}
\end{equation*}
and similarly for \(\sqC'\) and \(\phi'\). So Bayesian Descent is scale
invariant.

\fxnote{More invariants? Compare with Natural Gradient Descent?}