\begin{abstract}
	In our motivation we explain how current optimization schemes are based on
	the Taylor expansion and outline how the Taylor expansion leads to
	hyperparameters. We motivate why it is plausible to view loss functions as
	realizations of a random field and point to existing literature where this
	was done successfully in practice. Using statistic estimators to approximate
	this random loss field, we show that we can get an explicit learning rate for
	gradient descent in contrast to approximating the loss with the purely
	analytic ``Taylor estimator''.
\end{abstract}