\section{Best Linear Unbiased Estimator}

Now that we hopefully justified, why we want to view \(\Loss\) as a random field,
let us see what we gain from that. Bayesian optimization provides us with
an excellent but also very expensive answer to that question. We now want to
find a cheap approximation of \(\Loss\) again and optimize that instead.

Let us first recapitulate what a BLUE is. A \textbf{linear estimator}
\(\hat{Y}\) of \(Y\) using \(X_1,\dots,X_n\) is of the form 
\begin{equation*}
	\hat{Y}\in \linHull\{X_1,\dots,X_n\} + \real.
\end{equation*}
The set of \textbf{unbiased linear estimators} is defined as
\begin{align}\label{def: LUE}
	\LUE
	&= \{ \hat{Y} \in \linHull\{X_1,\dots,X_n\} + \real : \E[\hat{Y}] = \E[Y]\}\\
	\nonumber
	&= \{ \hat{Y} - \E[\hat{Y}] + \E[Y] : \hat{Y} \in \linHull\{X_1,\dots,X_n\}\}.
\end{align}
And the BLUE is the \textbf{best linear unbiased estimator}, i.e.
\begin{equation}\label{def: BLUE}
	\BLUE[Y\mid X_1,\dots, X_n] := \argmin_{\hat{Y}\in\LUE} \E[\|\hat{Y} - Y\|^2].
\end{equation}
Other risk functions to minimize are possible, but this is the usual one.

\input{snippets/blue_is_cond_expec.tex}
\fxwarning{double-check this statement, source?, appendix?}
\begin{remark}
	As Bayesian Optimization typically assumes Gaussian random fields, working with
	the BLUE is really equivalent to working with the conditional expectation in
	that setting.
\end{remark}

