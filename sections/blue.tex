\section{Best Linear Unbiased Estimator}

Now that we hopefully justified, why we want to view \(\Loss\) as a random field,
let us see what we gain from that. Bayesian optimization provides us with
an excellent but also very expensive answer to that question. We now want to
find a cheap approximation of \(\Loss\) again and optimize that instead.

Let us first recapitulate what a BLUE is. A linear estimator \(\hat{Y}\) of
\(Y\) using \(X_1,\dots,X_n\) is an element of the linear hull shifted by a
constant 
\begin{equation*}
	\hat{Y}\in \linHull\{X_1,\dots,X_n\} + \real.
\end{equation*}
An unbiased linear estimator is from the set
\begin{align}\label{def: LUE}
	\LUE
	&= \{ \hat{Y} \in \linHull\{X_1,\dots,X_n\} + \real : \E[\hat{Y}] = \E[Y]\}\\
	\nonumber
	&= \{ \hat{Y} - \E[\hat{Y}] + \E[Y] : \hat{Y} \in \linHull\{X_1,\dots,X_n\}\}.
\end{align}
And the BLUE is the best linear unbiased estimator from that set
\begin{equation}\label{def: BLUE}
	\BLUE[Y\mid X_1,\dots, X_n] := \argmin_{\hat{Y}\in\LUE} \E[\|\hat{Y} - Y\|^2].
\end{equation}
Other risk functions to minimize are possible, but this is the usual one.
\begin{lemma}\label{lem: blue is cond. expectation}
	If \(X,Y_1,\dots, Y_n\) are multivariate normal distributed, then we
	have\fxwarning{double-check this statement, source?, appendix?}
	\begin{align*}
		\BLUE[Y\mid X_1,\dots,X_n]
		&= \E[Y\mid X_1,\dots, X_n]\\
		\bigg(&=
		\argmin_{\hat{Y}\in\{f(X_1,\dots, X_n) : f\text{ meas.}\}} \E[\|Y-\hat{Y}\|^2]
		\bigg).
	\end{align*}
\end{lemma}
\begin{remark}
	As Bayesian Optimization typically assumes Gaussian random fields, working with
	the BLUE is really equivalent to working with the conditional expectation in
	that setting.
\end{remark}

\subsection{Zero Order BLUE}

The most simple BLUE one can think of is given by

\begin{lemma}[One-Step Zero-Order BLUE]\label{lem: one-step zero-order blue}
	For an arbitrary square integrable random field \(\Loss\) with covariance
	function
	\begin{equation}\label{def: covariance function}
		\C(x,y):= \Cov(\Loss(x), \Loss(y)),
	\end{equation}
	we have
	\begin{equation*}
		\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
		= \E[\Loss(\param + \step)]
		+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
		\big[\Loss(\param) - \E[\Loss(\param)]\big].
	\end{equation*}
\end{lemma}
\fxwarning*{proof}

This isn't a particularly useful approximation. If we were to assume constant
expectation \(\E[\Loss(\param)] = \mu\), then we have
\begin{equation}\label{eq: constant expectation simple blue}
	\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
	=\mu 
	+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
	\big[\Loss(\param) - \mu\big]
\end{equation}
Now if \(\Loss(\param) < \mu\), then minimizing the BLUE in
\eqref{eq: constant expectation simple blue} results in a maximization of
\(\C(\param+\step, \param)\). This will generally be just \(\step=0\) unless
there is some higher variance region. In other words: We are satisfied and
stop optimizing as soon as we get below the average \(\mu\).

Minimizing the BLUE here is even greedier/conservative than maximizing
expected improvement \eqref{eq: expected improvement}. While the expected
improvement takes into account the fact that we can just return to the previous
minimum and therefore forces the improvement to be greater \(0\), we are
essentially optimizing without the \((\cdot)^+\) here. I.e instead of
maximizing
\begin{equation*}
	\EI(\param) = \E\big[
			[\hat{\Loss}_n^*- \Loss(\param)]^+
			\bigm|
			\Loss(\param_1),\dots, \Loss(\param_n)
		\big]
\end{equation*}
we do the following (keep Lemma~\ref{lem: blue is cond. expectation} in mind)
\begin{equation*}
	\max_{\param} \E\big[
		\hat{\Loss}_n^*- \Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big]
	= \hat{\Loss}_n^* - \min_{\param} \E\big[
		\Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big].
\end{equation*}
So if we minimize the BLUE/conditional expectation directly, we essentially fear
regressing back to a worse state as well, making use even more exploration
averse.

The point of this section was to demonstrate that minimizing the
\(\BLUE\) is not a sophisticated optimization method. Yet, we will see in the
next section that it results in a gradient descent method with a specific
learning rate. The main reason why this method is interesting is, that it scales
much better with higher dimensions and is much cheaper. But there is also the
fact, that in a stochastic gradient setting we cannot simply go back to
the minimum so far because we do not know how big the real loss was at any step.
So in this sense it actually makes sense to fear regression to a worse state.


