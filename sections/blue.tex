\section{Best Linear Unbiased Estimator}

Now that we hopefully justified, why we want to view \(\Loss\) as a random field,
let us see what we gain from that. Bayesian optimization provides us with
an excellent but also very expensive answer to that question. We now want to
find a cheap approximation of \(\Loss\) again and optimize that instead.

Let us first recapitulate what a BLUE is. A linear estimator \(\hat{Y}\) of
\(Y\) using \(X_1,\dots,X_n\) is an element of the linear hull shifted by a
constant 
\begin{equation*}
	\hat{Y}\in \linHull\{X_1,\dots,X_n\} + \real.
\end{equation*}
An unbiased linear estimator is from the set
\begin{align}\label{def: LUE}
	\LUE
	&= \{ \hat{Y} \in \linHull\{X_1,\dots,X_n\} + \real : \E[\hat{Y}] = \E[Y]\}\\
	\nonumber
	&= \{ \hat{Y} - \E[\hat{Y}] + \E[Y] : \hat{Y} \in \linHull\{X_1,\dots,X_n\}\}.
\end{align}
And the BLUE is the best linear unbiased estimator from that set
\begin{equation}\label{def: BLUE}
	\BLUE[Y\mid X_1,\dots, X_n] := \argmin_{\hat{Y}\in\LUE} \E[\|\hat{Y} - Y\|^2].
\end{equation}
Other risk functions to minimize are thinkable, but this is the usual one.
\begin{lemma}\label{lem: blue is cond. expectation}
	If \(X,Y_1,\dots, Y_n\) are multivariate normal distributed, then we
	have\fxwarning{double-check this statement, source?, appendix?}
	\begin{align*}
		\BLUE[Y\mid X_1,\dots,X_n]
		&= \E[Y\mid X_1,\dots, X_n]\\
		\bigg(&=
		\argmin_{\hat{Y}\in\{f(X_1,\dots, X_n) : f\text{ meas.}\}} \E[\|Y-\hat{Y}\|^2]
		\bigg).
	\end{align*}
\end{lemma}
\begin{remark}
	As Bayesian Optimization typically assumes Gaussian random fields, working with
	the BLUE is really equivalent to working with the conditional expectation in
	that setting.
\end{remark}

\subsection{Zero Order BLUE}

The most simple BLUE one can think of is given by

\begin{lemma}[One-Step Zero-Order BLUE]\label{lem: one-step zero-order blue}
	We have
	\begin{equation*}
		\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
		= \E[\Loss(\param + \step)]
		+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
		\big[\Loss(\param) - \E[\Loss(\param)]\big],
	\end{equation*}
	where we define the covariance function \(\C\) to be
	\begin{equation*}
		\C(x,y):= \Cov(\Loss(x), \Loss(y)).
	\end{equation*}
\end{lemma}
\fxwarning*{proof}

This isn't a particularly useful approximation. If we were to assume constant
expectation \(\E[\Loss(\param)] = \mu\), then we have
\begin{equation}\label{eq: constant expectation simple blue}
	\BLUE[\Loss(\param + \step) \mid \Loss(\param)]
	=\mu 
	+ \frac{\C(\param+\step,\param)}{\C(\param, \param)}
	\big[\Loss(\param) - \mu\big]
\end{equation}
Now if \(\Loss(\param) < \mu\), then minimizing the BLUE in
\eqref{eq: constant expectation simple blue} results in a maximization of
\(\C(\param+\step, \param)\). This will generally be just \(\step=0\) unless
there is some higher variance region. In other words: We are satisfied and
stop optimizing as soon as we get below the average \(\mu\).

Minimizing the BLUE here is even greedier/conservative than maximizing
expected improvement \eqref{eq: expected improvement}. While the expected
improvement takes into account the fact that we can just return to the previous
minimum and therefore forces the improvement to be greater \(0\), we are
essentially optimizing without the \((\cdot)^+\) here. I.e instead of
maximizing
\begin{equation*}
	\EI(\param) = \E\big[
			[\hat{\Loss}_n^*- \Loss(\param)]^+
			\bigm|
			\Loss(\param_1),\dots, \Loss(\param_n)
		\big]
\end{equation*}
we do the following (keep Lemma~\ref{lem: blue is cond. expectation} in mind)
\begin{equation*}
	\max_{\param} \E\big[
		\hat{\Loss}_n^*- \Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big]
	= \hat{\Loss}_n^* - \min_{\param} \E\big[
		\Loss(\param)
		\bigm|
		\Loss(\param_1),\dots, \Loss(\param_n)
	\big].
\end{equation*}
So if we minimize the BLUE/conditional expectation directly, we essentially fear
regressing back to a worse state as well, making use even more exploration
averse. There are good reasons to be this risk averse in a machine learning
context
\begin{enumerate}
	\item we want to avoid overfitting, i.e. we don't want to actually find the
	global minimum
	\item if we use stochastic gradients (SGD), we do not know after the step
	whether we made an improvement or not. So we can not go back to the best
	point so far.
\end{enumerate}
In addition, the BLUE from Lemma~\ref{lem: one-step zero-order blue}
uses only the last step.

This is all nice and well, but why are we doing this? We have learned that we
can not really gain anything from optimizing the BLUE. Well, at least if we do
not include more information to construct our estimator...

\subsection{First Order BLUE}

What if we add the gradient to the BLUE and try again?
For that the following
statements are useful.
\begin{lemma}[Covariance of the Derivative]
	\label{lem: covariance of derivative}
	\input{theorems/lem:covariance_of_derivative.tex}
\end{lemma}

\begin{corollary}[Gaussian Case]
	If \(\Loss\) is a stationary gaussian random field, \(\Loss(x)\) and
	\(\nabla\Loss(x)\) are independent multivariate gaussian for every \(x\).
\end{corollary}

Due to this fact, the first order BLUE is greatly simplified by assuming a
stationary \(\Loss\). To simplify notation, we are also going to assume
the loss is centered in the following lemma.

\begin{lemma}[One-Step First-Order BLUE]
	If \(\Loss\) is a stationary, centred random field, then
	\begin{equation*}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \nabla\Loss(\param)]
		= \frac{\C(\step)}{\C(0)} \Loss(\param)
		+ [\nabla^2\C(0)]^{-1} \langle \nabla \C(\step), \nabla\Loss(\param)\rangle.
	\end{equation*}
	If we further assume that our random field is isotropic (not only shift
	invariant/stationary, but also rotation invariant), then we can
	write \(\C(\step)=\iC(\|\step\|^2)\), and get
	\begin{equation*}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \nabla\Loss(\param)]
		= \frac{\iC(\|\step\|^2)}{\iC(0)} \Loss(\param)
		+ \frac{\iC'(\|\step\|^2)}{\iC'(0)} \langle \step, \nabla\Loss(\param)\rangle.
	\end{equation*}
\end{lemma}
\begin{proof}
	As we are assuming a centred \(\Loss\), we know that any  element
	\(\hat{\Loss}(\param + \step)\) from \(\LUE\) has the form
	\begin{equation*}
		\hat{\Loss}(\param + \step)
		= a\Loss(\param) + \langle b, \nabla\Loss(\param)\rangle
	\end{equation*}
	for \(a\in\real\), \(b\in \real^\dimension\). So we want to minimize 
	\begin{align*}
		g(a,b)
		&=\E[\|\Loss(\param +\step) - \hat{\Loss}(\param + \step)\|^2]\\
		&= \begin{aligned}[t]
			&\E[\Loss(\param +\step)^2]\\
			&- 2a\E[\Loss(\param + \step)\Loss(\param)] + a^2 \E[\Loss(\param)^2]\\
			&- 2\langle b, \E[\Loss(\param + \step)\nabla\Loss(\param)]\rangle
			+ \langle b, \E[\nabla\Loss(\param)\nabla\Loss(\param)^T] b\rangle\\
			&- 2a\langle b, \underbrace{\E[\Loss(\param)\nabla\Loss(\param)]}_{=0}\rangle
		\end{aligned}\\
		&=\C(0) - 2a\C(\step) + a^2 \C(0)
		- 2\langle b, \nabla\C(\step)\rangle
		+ \langle b, \nabla^2\C(0) b\rangle
	\end{align*}
	with respect to \(a\) and \(b\). Fortunately there are no interactions
	between \(a\) and \(b\) as \(\Loss(\param)\) is uncorrelated from
	\(\nabla\Loss(\param)\). First order conditions result in
	\begin{equation*}
		a= \frac{\C(\step)}{\C(0)}
		\quad \text{and} \quad
		b = [\nabla^2 \C(0)]^{-1}\nabla \C(\step).
	\end{equation*}
	These are minima, since \(\nabla^2\C(0)\) is positive definite, because of
	\begin{align*}
		\langle b, \nabla^2\C(0) b\rangle = \E[\langle b, \nabla\Loss(\param)\rangle^2] \ge 0.
	\end{align*}
	Now we just need to address the \(\C(\step) = \iC(\|\step\|^2)\) case. We have
	\begin{align*}
		\nabla \C(\step) &= 2\iC'(\|\step\|^2) \step\\
		\nabla^2 \C(\step) &= 4 \iC''(\|\step\|^2) \step \step^T + 2\iC'(\|\step\|^2)\identity
	\end{align*}
	and therefore \(\nabla^2\C(0) = 2\iC'(0)\identity\). Plugging these results
	into the general formula yields the claim.
\end{proof}
\begin{remark}
	One could also drop the conditioning on the loss itself
	\begin{equation*}
		\BLUE[\Loss(\param + \step)\mid \nabla\Loss(\param)]
		= [\nabla^2\C(0)]^{-1}\langle\nabla\C(\step), \nabla\Loss(\param)\rangle
		= \frac{\iC'(\|\step\|^2)}{\iC'(0)}\langle \step, \nabla\Loss(\param)\rangle.
	\end{equation*}
	Then the estimator does not take the current height into account.
\end{remark}

\begin{corollary}
	Gradient descent, i.e.
	\begin{equation*}
		\step = - \tilde{\lr} \nabla\Loss(\param)
		= - \frac{\lr}{\|\nabla\Loss(\param)\|} \nabla\Loss(\param)
	\end{equation*}
	minimizes the 1-step 0-order \(\BLUE\) for an isotropic random
	field for some learning rate \(\tilde{\lr}\), i.e.
	\begin{equation*}
		\min_{\step}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \nabla\Loss(\param)]
		= \min_{\lr} \frac{\iC(\lr^2)}{\iC(0)} \Loss(\param)
		- \frac{\iC'(\lr^2)}{\iC'(0)} \lr \|\nabla\Loss(\param)\|.
	\end{equation*}
\end{corollary}
\begin{proof}
	Essentially all the occurrences of \(\step\) in our \(\BLUE\) are
	rotation invariant, so only the step size matters. The only place where that
	is not the case is the scalar product with the gradient. Therefore it
	determines the direction, i.e.
	\begin{align*}
		&\min_{\step}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \nabla\Loss(\param)]\\
		&= \min_{\lr}\min_{\step:\|\step\|=\lr}
		\BLUE[\Loss(\param + \step)\mid \Loss(\param), \nabla\Loss(\param)]\\
		&= \min_{\lr}\min_{\step:\|\step\|=\lr}
		\frac{\iC(\lr^2)}{\iC(0)} \Loss(\param)
		+ \frac{\iC'(\lr^2)}{\iC'(0)} \langle \step, \nabla\Loss(\param)\rangle\\
		&= \min_{\lr}
		\frac{\iC(\lr^2)}{\iC(0)} \Loss(\param)
		+ \frac{\iC'(\lr^2)}{\iC'(0)}
		\underbrace{\min_{\step:\|\step\|=\lr}\langle \step, \nabla\Loss(\param)\rangle}_{
			= -\eta \|\nabla \Loss(\param)\| \quad \mathrlap{\text{(optimal by Cauchy-Schwarz)}}
		}.\hphantom{\text{by Cauchy-Schwarz}}
	\end{align*}
	When moving the minimum into the equation we assumed that
	\(\frac{\iC'(\lr^2)}{\iC'(0)}\ge 0\). But if it was smaller zero, we would get a
	maximization instead, resulting in gradient descent with a negative learning
	rate.
\end{proof}




