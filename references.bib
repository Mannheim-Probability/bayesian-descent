
@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  number = {15},
  eprint = {cond-mat/0611023},
  eprinttype = {arxiv},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces2.pdf;/Users/felix/Zotero/storage/RV2A6TST/0611023.html}
}

@inproceedings{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html},
  urldate = {2022-06-10},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  file = {/Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf;/Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional2.pdf;/Users/felix/Zotero/storage/ZICT2QCI/1406.html}
}

@thesis{duvenaudAutomaticModelConstruction2014,
  type = {phdthesis},
  title = {Automatic Model Construction with {{Gaussian}} Processes},
  author = {Duvenaud, David},
  date = {2014},
  institution = {{University of Cambridge}},
  file = {/Users/felix/paper/2014_Duvenaud/Duvenaud_2014_Automatic model construction with Gaussian processes.pdf}
}

@incollection{frazierBayesianOptimization2018,
  title = {Bayesian {{Optimization}}},
  booktitle = {Recent {{Advances}} in {{Optimization}} and {{Modeling}} of {{Contemporary Problems}}},
  author = {Frazier, Peter I.},
  date = {2018-10},
  series = {{{INFORMS TutORials}} in {{Operations Research}}},
  eprint = {1807.02811},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {255--278},
  publisher = {{INFORMS}},
  doi = {10.1287/educ.2018.0188},
  archiveprefix = {arXiv},
  isbn = {978-0-9906153-2-3},
  keywords = {Bayesian optimization,Computer Science - Machine Learning,derivative-free optimization,entropy search,expected improvement,Gaussian processes,knowledge-gradient methods,Mathematics - Optimization and Control,optimization of expensive functions,Statistics - Machine Learning,surrogate-based optimization},
  file = {/Users/felix/paper/2018_Frazier/Frazier_2018_A Tutorial on Bayesian Optimization.pdf;/Users/felix/paper/2018_Frazier/Frazier_2018_Bayesian Optimization.pdf}
}

@book{nealBayesianLearningNeural1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  date = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  edition = {1},
  volume = {118},
  publisher = {{Springer New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  isbn = {978-0-387-94724-2},
  langid = {english},
  pagetotal = {204},
  file = {/Users/felix/paper/1996_Neal/Neal_1996_Bayesian Learning for Neural Networks.pdf;/Users/felix/Zotero/storage/GC37WL36/978-1-4612-0745-0.html}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2018},
  series = {Springer Optimization and {{Its}} Applications; Volume 137},
  edition = {Second edition},
  publisher = {{Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics, Introduction -- Part I Black-Box Optimization -- 1 Nonlinear Optimization -- 2 Smooth Convex Optimization -- 3 Nonsmooth Convex Optimization -- 4 Second-Order Methods -- Part II Structural Optimization -- 5 Polynomial-time Interior-Point Methods -- 6 Primal-Dual Model of Objective Function -- 7 Optimization in Relative Scale -- Bibliographical Comments -- Appendix A. Solving some Auxiliary Optimization Problems -- References -- Index},
  isbn = {978-3-319-91578-4},
  langid = {english},
  keywords = {Computer software; Optimization; Mathematical optimization; Algorithms,Konvexe Optimierung},
  file = {/Users/felix/paper/2018_Nesterov/Nesterov_2018_Lectures on Convex Optimization.pdf}
}

@unpublished{pascanuSaddlePointProblem2014,
  title = {On the Saddle Point Problem for Non-Convex Optimization},
  author = {Pascanu, Razvan and Dauphin, Yann N. and Ganguli, Surya and Bengio, Yoshua},
  date = {2014-05-27},
  eprint = {1405.4604},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1405.4604},
  urldate = {2021-09-16},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/felix/paper/2014_Pascanu et al/Pascanu et al_2014_On the saddle point problem for non-convex optimization.pdf;/Users/felix/Zotero/storage/7B28YP5G/1405.html}
}

@thesis{scheurerComparisonModelsMethods2009,
  title = {A {{Comparison}} of {{Models}} and {{Methods}} for {{Spatial Interpolation}} in {{Statistics}} and {{Numerical Analysis}}},
  author = {Scheurer, Michael},
  date = {2009},
  institution = {{Göttingen}},
  doi = {10.53846/goediss-2461},
  langid = {english},
  pagetotal = {142}
}

@article{schlatherParametricModelBridging2017,
  title = {A Parametric Model Bridging between Bounded and Unbounded Variograms},
  author = {Schlather, Martin and Moreva, Olga},
  date = {2017},
  journaltitle = {Stat},
  volume = {6},
  number = {1},
  pages = {47--52},
  issn = {2049-1573},
  doi = {10.1002/sta4.134},
  abstract = {A simple variogram model with two parameters is presented that includes the power variogram for fractional Brownian motion, a modified De Wijsian model, the generalized Cauchy model and the multiquadric model. One parameter controls the sample path roughness of the process. The other parameter allows for a smooth transition between bounded and unbounded variograms, that is, between stationary and intrinsically stationary processes in a Gaussian framework, or between mixing and non-ergodic Brown–Resnick processes when modeling spatial extremes. Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {geostatistics,simulation,spatial statistics,stochastic processes},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.134},
  file = {/Users/felix/paper/2017_Schlather_Moreva/Schlather_Moreva_2017_A parametric model bridging between bounded and unbounded variograms.pdf;/Users/felix/Zotero/storage/QV7VKKRM/sta4.html}
}

@unpublished{truongBacktrackingGradientDescent2019,
  title = {Backtracking Gradient Descent Method for General $C^1$ Functions, with Applications to {{Deep Learning}}},
  author = {Truong, Tuyen Trung and Nguyen, Tuan Hang},
  date = {2019-04-04},
  eprint = {1808.05160},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1808.05160},
  urldate = {2021-09-27},
  abstract = {While Standard gradient descent is one very popular optimisation method, its convergence cannot be proven beyond the class of functions whose gradient is globally Lipschitz continuous. As such, it is not actually applicable to realistic applications such as Deep Neural Networks. In this paper, we prove that its backtracking variant behaves very nicely, in particular convergence can be shown for all Morse functions. The main theoretical result of this paper is as follows. Theorem. Let \$f:\textbackslash mathbb\{R\}\^k\textbackslash rightarrow \textbackslash mathbb\{R\}\$ be a \$C\^1\$ function, and \$\textbackslash\{z\_n\textbackslash\}\$ a sequence constructed from the Backtracking gradient descent algorithm. (1) Either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_\{n+1\}-z\_n||=0\$. (2) Assume that \$f\$ has at most countably many critical points. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ converges to a critical point of \$f\$. (3) More generally, assume that all connected components of the set of critical points of \$f\$ are compact. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ is bounded. Moreover, in the latter case the set of cluster points of \$\textbackslash\{z\_n\textbackslash\}\$ is connected. Some generalised versions of this result, including an inexact version, are included. Another result in this paper concerns the problem of saddle points. We then present a heuristic argument to explain why Standard gradient descent method works so well, and modifications of the backtracking versions of GD, MMT and NAG. Experiments with datasets CIFAR10 and CIFAR100 on various popular architectures verify the heuristic argument also for the mini-batch practice and show that our new algorithms, while automatically fine tuning learning rates, perform better than current state-of-the-art methods such as MMT, NAG, Adagrad, Adadelta, RMSProp, Adam and Adamax.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2019_Truong_Nguyen/Truong_Nguyen_2019_Backtracking gradient descent method for general $C^1$ functions, with.pdf;/Users/felix/Zotero/storage/43XEVGBC/1808.html}
}

@book{williamsGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Williams, Christopher KI and Rasmussen, Carl Edward},
  date = {2006},
  series = {Adaptive Computation and Machine Learning},
  edition = {2},
  number = {3},
  publisher = {{MIT press Cambridge, MA}},
  url = {http://gaussianprocess.org/gpml/chapters/RW.pdf},
  isbn = {0-262-18253-X},
  langid = {english},
  pagetotal = {248},
  file = {/Users/felix/paper/2006_Williams_Rasmussen/Williams_Rasmussen_2006_Gaussian processes for machine learning2.pdf}
}

@inproceedings{wuBayesianOptimizationGradients2017,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html},
  urldate = {2022-06-02},
  abstract = {Bayesian optimization has shown success in global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. dKG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the dKG acquisition function and its gradient using a novel fast discretization-free technique. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  file = {/Users/felix/paper/2017_Wu et al/Wu et al_2017_Bayesian Optimization with Gradients.pdf}
}


